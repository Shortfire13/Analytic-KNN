{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-12 06:48:56+00:00</td>\n",
       "      <td>rxxyz6</td>\n",
       "      <td>@FiersaBesari terlalu menyakitkan ketika diliat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-12 06:28:05+00:00</td>\n",
       "      <td>nadubasyri</td>\n",
       "      <td>SEBUAH GARIS WAKTU...\\n\\nJika saatnya tiba sed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-12 06:27:33+00:00</td>\n",
       "      <td>missheart222</td>\n",
       "      <td>@t @teddspotting @R @jericho_rosales @rajolaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-12 06:10:41+00:00</td>\n",
       "      <td>daissysfriend</td>\n",
       "      <td>fiersa besari was right,\\n\\nkita adalah rasa y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-12 06:02:46+00:00</td>\n",
       "      <td>Selvi28874608</td>\n",
       "      <td>@FiersaBesari ðŸ¤£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime       username  \\\n",
       "0  2022-10-12 06:48:56+00:00         rxxyz6   \n",
       "1  2022-10-12 06:28:05+00:00     nadubasyri   \n",
       "2  2022-10-12 06:27:33+00:00   missheart222   \n",
       "3  2022-10-12 06:10:41+00:00  daissysfriend   \n",
       "4  2022-10-12 06:02:46+00:00  Selvi28874608   \n",
       "\n",
       "                                             content  \n",
       "0    @FiersaBesari terlalu menyakitkan ketika diliat  \n",
       "1  SEBUAH GARIS WAKTU...\\n\\nJika saatnya tiba sed...  \n",
       "2  @t @teddspotting @R @jericho_rosales @rajolaur...  \n",
       "3  fiersa besari was right,\\n\\nkita adalah rasa y...  \n",
       "4                                    @FiersaBesari ðŸ¤£  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('dataset1.csv')#ubah nama file sesai dengan nama file kalian\n",
    "    return data\n",
    "\n",
    "tweet_df = load_data()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisi dataframe\n",
    "df  = pd.DataFrame(tweet_df[['datetime','username', 'content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      terlalu menyakitkan ketika diliat\n",
       "1      SEBUAH GARIS WAKTU...\\n\\nJika saatnya tiba sed...\n",
       "2       eddspotting          am_concepcion   eddspott...\n",
       "3      fiersa besari was right,\\n\\nkita adalah rasa y...\n",
       "4                                                      ðŸ¤£\n",
       "                             ...                        \n",
       "995                                 Rada rada maksa yaaðŸ˜­\n",
       "996      semogaa remedial kali ini dapett hasil yang ...\n",
       "997     Satru 2 - Denny caknan\\nDan - So7\\nRuntuh - F...\n",
       "998                                 mungkin hrs remedial\n",
       "999                     Ikut remedial juga boleh gk kak,\n",
       "Name: remove_user, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove user \n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "df['remove_user'] = np.vectorize(remove_pattern)(df['content'], \"@[\\w]*\")\n",
    "df['remove_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove user\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "df['remove_user'] = np.vectorize(remove_pattern)(df['content'], \"@[\\w]*\")\n",
    "\n",
    "def remove(tweet):\n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    return tweet\n",
    "df['remove_http'] = df['remove_user'].apply(lambda x: remove(x))\n",
    "df.sort_values(\"remove_http\", inplace = True)\n",
    "df.drop_duplicates(subset =\"remove_http\", keep = 'first', inplace = True)\n",
    "df['remove_http'].to_csv('after_cleaning.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopword\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_indonesia = stopwords.words('indonesian')\n",
    " \n",
    "#import sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    #remove coma\n",
    "    tweet = re.sub(r',','',tweet)\n",
    "    \n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "     # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_indonesia and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "df['tweet_clean'] = df['remove_http'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[abang, nggak, kenal, bimantara, humpuss, citr...</td>\n",
       "      <td>abang nggak kenal bimantara humpuss citra lamt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>[abunawas, laen, jokowi boong, g, merugika, ra...</td>\n",
       "      <td>abunawas laen jokowi boong g merugika rakyat j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>[abunawas, pas]</td>\n",
       "      <td>abunawas pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>[adil, pertanggungjawaban, buka]</td>\n",
       "      <td>adil pertanggungjawaban buka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>[yth, presiden, kurang, inflasi, qbat, bbm, bb...</td>\n",
       "      <td>yth presiden kurang inflasi qbat bbm bbm tdk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>[yth, presiden, mohon, maaf, sih, urgensi, ban...</td>\n",
       "      <td>yth presiden mohon maaf sih urgensi bangun ikn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>[yth, selamat, pilih, bpk, abdu, selamat, tuga...</td>\n",
       "      <td>yth selamat pilih bpk abdu selamat tugas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>[yuk, dkasih, ruang, utk, yg]</td>\n",
       "      <td>yuk dkasih ruang utk yg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>[zaman, jokowi, bikin, rame, bikin, gaduh, , ,...</td>\n",
       "      <td>zaman jokowi bikin rame bikin gaduh zaman lelu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_clean  \\\n",
       "274                                                 []   \n",
       "55   [abang, nggak, kenal, bimantara, humpuss, citr...   \n",
       "649  [abunawas, laen, jokowi boong, g, merugika, ra...   \n",
       "578                                    [abunawas, pas]   \n",
       "626                   [adil, pertanggungjawaban, buka]   \n",
       "..                                                 ...   \n",
       "727  [yth, presiden, kurang, inflasi, qbat, bbm, bb...   \n",
       "348  [yth, presiden, mohon, maaf, sih, urgensi, ban...   \n",
       "751  [yth, selamat, pilih, bpk, abdu, selamat, tuga...   \n",
       "875                      [yuk, dkasih, ruang, utk, yg]   \n",
       "850  [zaman, jokowi, bikin, rame, bikin, gaduh, , ,...   \n",
       "\n",
       "                                                 Tweet  \n",
       "274                                                     \n",
       "55   abang nggak kenal bimantara humpuss citra lamt...  \n",
       "649  abunawas laen jokowi boong g merugika rakyat j...  \n",
       "578                                       abunawas pas  \n",
       "626                       adil pertanggungjawaban buka  \n",
       "..                                                 ...  \n",
       "727  yth presiden kurang inflasi qbat bbm bbm tdk t...  \n",
       "348  yth presiden mohon maaf sih urgensi bangun ikn...  \n",
       "751           yth selamat pilih bpk abdu selamat tugas  \n",
       "875                            yuk dkasih ruang utk yg  \n",
       "850  zaman jokowi bikin rame bikin gaduh zaman lelu...  \n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punct\n",
    "def remove_punct(text):\n",
    "    text  = \" \".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "df['Tweet'] = df['tweet_clean'].apply(lambda x: remove_punct(x))\n",
    " #=========================================================================#\n",
    "df.sort_values(\"Tweet\", inplace = True)\n",
    "df.drop(df.columns[[0,1,2,3,4]], axis = 1, inplace = True)\n",
    "df.drop_duplicates(subset =\"Tweet\", keep = 'first', inplace = True)\n",
    "df.to_csv('output.csv',encoding='utf8', index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb806f72044836682993c29a82379a3b0c4860545a1443ac53bc5b71762b6c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
