{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76d5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# import translators as ts\n",
    "# import translators.server as tss\n",
    "# from deep_translator import (GoogleTranslator,\n",
    "#                              MicrosoftTranslator,\n",
    "#                              PonsTranslator,\n",
    "#                              LingueeTranslator,\n",
    "#                              MyMemoryTranslator,\n",
    "#                              YandexTranslator,\n",
    "#                              PapagoTranslator,\n",
    "#                              DeeplTranslator,\n",
    "#                              QcriTranslator,\n",
    "#                              single_detection,\n",
    "#                              batch_detection)\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# %matplotlib inline\n",
    "# pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75def54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('testing.csv')#ubah nama file sesai dengan nama file kalian\n",
    "    return data\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "#definisi dataframe\n",
    "df  = pd.DataFrame(df[['Account','caption']])#ubah dataframe sesuai column kalian\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# df.to_csv('dataset fix.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9af8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================Cleansing=====================================#\n",
    "#remove usero\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "df['remove_user'] = np.vectorize(remove_pattern)(df['caption'], \"@[\\w]*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def casefolding(content):\n",
    "    content = content.lower()\n",
    "    content = content.strip(\" \")\n",
    "    content = re.sub(r'[?|.|!_:\")(-+,#]','', content)\n",
    "    return content\n",
    "df['casefolding'] = df['caption'].apply(casefolding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcff866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate(text):\n",
    "#     # trans = GoogleTranslator(source='auto', target='id').translate_batch(text)\n",
    "#     # trans = DeeplTranslator(\"fa0d9ed62c9942f8c17cca30246c5aa9\").translate_batch(text)\n",
    "#     # trans = LingueeTranslator(source='auto', target='id').translate_words(text)\n",
    "#     trans = tss.google(text, from_language=\"en\", to_language=\"id\")\n",
    "#     return trans\n",
    "\n",
    "# df['translate'] = df['casefolding'].apply(translate)\n",
    "# df['translate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a22f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #=================================Stopword========================================#\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def stop(text):\n",
    "    text = text.lower()\n",
    "    factory = StopWordRemoverFactory()\n",
    "    more_stopword = ['dengan', 'ia','bahwa','oleh', 'dan', 'bagi', 'lu', 'gw', 'yang', 'tapi']\n",
    "    stopword= factory.create_stop_word_remover()\n",
    "    out = stopword.remove(text)\n",
    "    return out\n",
    "\n",
    "df['stopword'] = df['casefolding'].apply(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "275d0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================Tokenizing========================================#\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def token(tweet):\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "\n",
    "df['token'] = df['stopword'].apply(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fca62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symclean(text):\n",
    "    tokens = text\n",
    "    # remove all tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    return words\n",
    "df['remove_sym'] = df['token'].apply(symclean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6c4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f48cbbc4f6476c8cbaface1a545763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in df['remove_sym']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    \n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "df['stemming'] = df['remove_sym'].swifter.apply(get_stemmed_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd5b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final</th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promo kode gaji laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>promo kode gaji laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>promo kode gaji laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>promo kode gaji laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>promo kode gaji laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>promo kode june laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>promo kode june laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>promo kode june laravel https wwwudemycom cour...</td>\n",
       "      <td>[0.017699115044247787, 0.017699115044247787, 0...</td>\n",
       "      <td>[1.4944125941246607, 1.4944125941246607, 1.494...</td>\n",
       "      <td>[0.026449780426985144, 0.026449780426985144, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>java logging udah php logging udah golang logg...</td>\n",
       "      <td>[0.058823529411764705, 0.058823529411764705, 0...</td>\n",
       "      <td>[1.4489502200479032, 1.4601235206460286, 1.482...</td>\n",
       "      <td>[0.08523236588517077, 0.0858896188615311, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>fun fact ajar java gara salah nitip buku dibel...</td>\n",
       "      <td>[0.07142857142857142, 0.07142857142857142, 0.2...</td>\n",
       "      <td>[3.751535313041949, 5.255612709818223, 5.94875...</td>\n",
       "      <td>[0.2679668080744249, 0.37540090784415875, 1.27...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 final  \\\n",
       "0    promo kode gaji laravel https wwwudemycom cour...   \n",
       "1    promo kode gaji laravel https wwwudemycom cour...   \n",
       "2    promo kode gaji laravel https wwwudemycom cour...   \n",
       "3    promo kode gaji laravel https wwwudemycom cour...   \n",
       "4    promo kode gaji laravel https wwwudemycom cour...   \n",
       "..                                                 ...   \n",
       "136  promo kode june laravel https wwwudemycom cour...   \n",
       "137  promo kode june laravel https wwwudemycom cour...   \n",
       "138  promo kode june laravel https wwwudemycom cour...   \n",
       "139  java logging udah php logging udah golang logg...   \n",
       "140  fun fact ajar java gara salah nitip buku dibel...   \n",
       "\n",
       "                                                    TF  \\\n",
       "0    [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "1    [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "2    [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "3    [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "4    [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "..                                                 ...   \n",
       "136  [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "137  [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "138  [0.017699115044247787, 0.017699115044247787, 0...   \n",
       "139  [0.058823529411764705, 0.058823529411764705, 0...   \n",
       "140  [0.07142857142857142, 0.07142857142857142, 0.2...   \n",
       "\n",
       "                                                   IDF  \\\n",
       "0    [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "1    [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "2    [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "3    [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "4    [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "..                                                 ...   \n",
       "136  [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "137  [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "138  [1.4944125941246607, 1.4944125941246607, 1.494...   \n",
       "139  [1.4489502200479032, 1.4601235206460286, 1.482...   \n",
       "140  [3.751535313041949, 5.255612709818223, 5.94875...   \n",
       "\n",
       "                                                 TFIDF  \n",
       "0    [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "1    [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "2    [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "3    [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "4    [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "..                                                 ...  \n",
       "136  [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "137  [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "138  [0.026449780426985144, 0.026449780426985144, 0...  \n",
       "139  [0.08523236588517077, 0.0858896188615311, 0.08...  \n",
       "140  [0.2679668080744249, 0.37540090784415875, 1.27...  \n",
       "\n",
       "[141 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "max_features = 1000\n",
    "\n",
    "def join(text):\n",
    "    string =' '.join(map(str,text))\n",
    "    return string\n",
    "\n",
    "df['final'] = df['stemming'].apply(join)\n",
    "# dfs = df['final', 'sentiment'].to_csv('final.csv')\n",
    "\n",
    "def generate_tfidf_mat(min_gram, max_gram):\n",
    "    cvect = CountVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram))\n",
    "    counts = cvect.fit_transform(df[\"final\"])\n",
    "\n",
    "    normalized_counts = normalize(counts, norm='l1', axis=1)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram), smooth_idf=False)\n",
    "    tfs = tfidf.fit_transform(df[\"final\"])\n",
    "\n",
    "    tfidf_mat = normalized_counts.multiply(tfidf.idf_).toarray()\n",
    "    \n",
    "    TF = normalized_counts.toarray()\n",
    "    IDF = tfidf.idf_\n",
    "    TF_IDF = tfidf_mat\n",
    "    return TF, IDF, TF_IDF, tfidf.get_feature_names()\n",
    "\n",
    "\n",
    "#test\n",
    "\n",
    "\n",
    "# ngram_range (1, 1) to use unigram only\n",
    "tf_mat_unigram, idf_mat_unigram, tfidf_mat_unigram, terms_unigram = generate_tfidf_mat(1,1)\n",
    "\n",
    "# ngram_range (2, 2) to use bigram only\n",
    "tf_mat_bigram, idf_mat_bigram, tfidf_mat_bigram, terms_bigram = generate_tfidf_mat(2,2)\n",
    "\n",
    "# ngram_range (3, 3) to use trigram only\n",
    "tf_mat_trigram, idf_mat_trigram, tfidf_mat_trigram, terms_trigram = generate_tfidf_mat(3,3)\n",
    "\n",
    "# # ---------- check sparse data -------------------\n",
    "# idx_sample = 0\n",
    "\n",
    "# print(\"Show TFIDF sample ke-\" + str(idx_sample), \"\\n\")\n",
    "# print(df[\"final\"][idx_sample], \"\\n\")\n",
    "\n",
    "# print(\"\\t\\t\\t\", \"TF\", \"\\t\\t\", \"IDF\", \"\\t\\t\", \"TF-IDF\", \"\\t\", \"Term\\n\")\n",
    "# for i, item in enumerate(zip(tf_mat_unigram[idx_sample], idf_mat_unigram, tfidf_mat_unigram[idx_sample], terms_unigram)):\n",
    "#     if(item[2] != 0.0):\n",
    "#         print (\"array position \" + str(i) + \"\\t\", \n",
    "#                \"%.6f\" % item[0], \"\\t\", \n",
    "#                \"%.6f\" % item[1], \"\\t\", \n",
    "#                \"%.6f\" % item[2], \"\\t\", \n",
    "#                item[3])\n",
    "        \n",
    "def get_TF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [tf for tf in tf_mat_unigram[idx] if tf != 0.0]\n",
    "\n",
    "df[\"TF\"] = df.apply(get_TF_unigram, axis=1)\n",
    "\n",
    "def get_IDF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [item[1] for item in zip(tf_mat_unigram[idx], idf_mat_unigram) if item[0] != 0.0]\n",
    "\n",
    "df[\"IDF\"] = df.apply(get_IDF_unigram, axis=1)\n",
    "\n",
    "def get_TFIDF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [tfidf for tfidf in tfidf_mat_unigram[idx] if tfidf != 0.0]\n",
    "\n",
    "df[\"TFIDF\"] = df.apply(get_TFIDF_unigram, axis=1)\n",
    "\n",
    "df[[\"final\", \"TF\", \"IDF\", \"TFIDF\"]]\n",
    "\n",
    "# save TFIDF Unigram to Excel\n",
    "\n",
    "# df[[\"final\", \"TF_UNIGRAM\", \"IDF_UNIGRAM\", \"TFIDF_UNIGRAM\"]].to_csv(\"TFIDF_Unigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c1cf33a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Kuliah\\belajar\\Project\\Machine Learning\\Analytic-KNN\\Pre.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m X_test_counts \u001b[39m=\u001b[39m count_vect\u001b[39m.\u001b[39mfit_transform(\u001b[39mlist\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m X_test \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(\u001b[39mlist\u001b[39;49m(X_test_counts))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y_test \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Data\n",
    "def data_train():\n",
    "    data = pd.read_csv('label.csv')#ubah nama file sesai dengan nama file kalian\n",
    "    return data\n",
    "df_train = data_train()\n",
    "#definisi dataframe\n",
    "df_train = df_train.rename(columns={'final': 'caption'})\n",
    "df_train  = pd.DataFrame(df_train[['caption','sentiment']])\n",
    "df_train = df_train.dropna().reset_index(drop=True)#ubah dataframe sesuai column kalian\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(df_train[\"caption\"])\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(df_train[\"caption\"])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "df_train['encoding'] = enc.fit_transform(df_train['sentiment'])\n",
    "y_train = df_train['encoding']\n",
    "\n",
    "\n",
    "#Test Data\n",
    "# X_test_counts = count_vect.fit_transform(df[\"final\"])\n",
    "tfidf = TfidfVectorizer()\n",
    "X_test = tfidf.fit_transform(df[\"final\"])\n",
    "df['sentiment'] = ''\n",
    "y_test = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd2cbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2687, 19616) (2687,) (141, 398) (141,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(tfs, target, test_size=0.2, stratify=target, random_state=85)\n",
    "print ((X_train.shape),(y_train.shape),(X_test.shape), (y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afddbc3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 398 features, but KNeighborsClassifier is expecting 19616 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Kuliah\\belajar\\Project\\Machine Learning\\Analytic-KNN\\Pre.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m knn \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# n_neighbors means k\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m knn\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Y_pred \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39minverse_transform(Y_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Kuliah/belajar/Project/Machine%20Learning/Analytic-KNN/Pre.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m (pred)\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:226\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    224\u001b[0m     \u001b[39m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[39m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    227\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_base.py:745\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    743\u001b[0m         X \u001b[39m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    744\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m         X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    747\u001b[0m n_samples_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_fit_\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m n_neighbors \u001b[39m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\faizi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 398 features, but KNeighborsClassifier is expecting 19616 features as input."
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "#KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)  # n_neighbors means k\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "pred = enc.inverse_transform(Y_pred)\n",
    "print (pred)\n",
    "# predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# # evaluate predictions\n",
    "# accuracy = accuracy_score(Y_test, predictions)\n",
    "# accuracies['KNN'] = accuracy* 100.0\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# #try to find best k value\n",
    "# scoreList = []\n",
    "# for i in range(1,20):\n",
    "#     knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "#     knn2.fit(X_train, Y_train)\n",
    "#     scoreList.append(knn2.score(X_test, Y_test))\n",
    "    \n",
    "# plt.plot(range(1,20), scoreList)\n",
    "# plt.xticks(np.arange(1,20,1))\n",
    "# plt.xlabel(\"K value\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.show()\n",
    "\n",
    "# acc = max(scoreList)*100\n",
    "\n",
    "# print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "cluster = MongoClient(\"mongodb://db_hrcerdas:hrcerdas@hrcerdas.id:27017/?authMechanism=SCRAM-SHA-256\")\n",
    "db = cluster[\"testing\"]\n",
    "collection = db[\"testing\"]\n",
    "\n",
    "\n",
    "\n",
    "# results = collection.insert_many(df.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0rc2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "37c0448c1c422973ba8fdc39db167944d77591df89dc93671090cd0e296c9cc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
