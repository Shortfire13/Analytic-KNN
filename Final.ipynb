{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# import translators as ts\n",
    "# import translators.server as tss\n",
    "# from deep_translator import (GoogleTranslator,\n",
    "#                              MicrosoftTranslator,\n",
    "#                              PonsTranslator,\n",
    "#                              LingueeTranslator,\n",
    "#                              MyMemoryTranslator,\n",
    "#                              YandexTranslator,\n",
    "#                              PapagoTranslator,\n",
    "#                              DeeplTranslator,\n",
    "#                              QcriTranslator,\n",
    "#                              single_detection,\n",
    "#                              batch_detection)\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# %matplotlib inline\n",
    "# pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75def54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('testing.csv')#ubah nama file sesai dengan nama file kalian\n",
    "    return data\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "#definisi dataframe\n",
    "df  = pd.DataFrame(df[['Account','caption']])#ubah dataframe sesuai column kalian\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# df.to_csv('dataset fix.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================Cleansing=====================================#\n",
    "#remove usero\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    \n",
    "df['remove_user'] = np.vectorize(remove_pattern)(df['caption'], \"@[\\w]*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def casefolding(content):\n",
    "    content = content.lower()\n",
    "    content = content.strip(\" \")\n",
    "    content = re.sub(r'[?|.|!_:\")(-+,#]','', content)\n",
    "    return content\n",
    "df['casefolding'] = df['caption'].apply(casefolding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate(text):\n",
    "#     # trans = GoogleTranslator(source='auto', target='id').translate_batch(text)\n",
    "#     # trans = DeeplTranslator(\"fa0d9ed62c9942f8c17cca30246c5aa9\").translate_batch(text)\n",
    "#     # trans = LingueeTranslator(source='auto', target='id').translate_words(text)\n",
    "#     trans = tss.google(text, from_language=\"en\", to_language=\"id\")\n",
    "#     return trans\n",
    "\n",
    "# df['translate'] = df['casefolding'].apply(translate)\n",
    "# df['translate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #=================================Stopword========================================#\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def stop(text):\n",
    "    text = text.lower()\n",
    "    factory = StopWordRemoverFactory()\n",
    "    more_stopword = ['dengan', 'ia','bahwa','oleh', 'dan', 'bagi', 'lu', 'gw', 'yang', 'tapi']\n",
    "    stopword= factory.create_stop_word_remover()\n",
    "    out = stopword.remove(text)\n",
    "    return out\n",
    "\n",
    "df['stopword'] = df['casefolding'].apply(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================Tokenizing========================================#\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def token(tweet):\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "\n",
    "df['token'] = df['stopword'].apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symclean(text):\n",
    "    tokens = text\n",
    "    # remove all tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    return words\n",
    "df['remove_sym'] = df['token'].apply(symclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in df['remove_sym']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    \n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "df['stemming'] = df['remove_sym'].swifter.apply(get_stemmed_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "max_features = 1000\n",
    "\n",
    "def join(text):\n",
    "    string =' '.join(map(str,text))\n",
    "    return string\n",
    "\n",
    "df['final'] = df['stemming'].apply(join)\n",
    "# dfs = df['final', 'sentiment'].to_csv('final.csv')\n",
    "\n",
    "def generate_tfidf_mat(min_gram, max_gram):\n",
    "    cvect = CountVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram))\n",
    "    counts = cvect.fit_transform(df[\"final\"])\n",
    "\n",
    "    normalized_counts = normalize(counts, norm='l1', axis=1)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram), smooth_idf=False)\n",
    "    tfs = tfidf.fit_transform(df[\"final\"])\n",
    "\n",
    "    tfidf_mat = normalized_counts.multiply(tfidf.idf_).toarray()\n",
    "    \n",
    "    TF = normalized_counts.toarray()\n",
    "    IDF = tfidf.idf_\n",
    "    TF_IDF = tfidf_mat\n",
    "    return TF, IDF, TF_IDF, tfidf.get_feature_names()\n",
    "\n",
    "\n",
    "#test\n",
    "\n",
    "\n",
    "# ngram_range (1, 1) to use unigram only\n",
    "tf_mat_unigram, idf_mat_unigram, tfidf_mat_unigram, terms_unigram = generate_tfidf_mat(1,1)\n",
    "\n",
    "# ngram_range (2, 2) to use bigram only\n",
    "tf_mat_bigram, idf_mat_bigram, tfidf_mat_bigram, terms_bigram = generate_tfidf_mat(2,2)\n",
    "\n",
    "# ngram_range (3, 3) to use trigram only\n",
    "tf_mat_trigram, idf_mat_trigram, tfidf_mat_trigram, terms_trigram = generate_tfidf_mat(3,3)\n",
    "\n",
    "# # ---------- check sparse data -------------------\n",
    "# idx_sample = 0\n",
    "\n",
    "# print(\"Show TFIDF sample ke-\" + str(idx_sample), \"\\n\")\n",
    "# print(df[\"final\"][idx_sample], \"\\n\")\n",
    "\n",
    "# print(\"\\t\\t\\t\", \"TF\", \"\\t\\t\", \"IDF\", \"\\t\\t\", \"TF-IDF\", \"\\t\", \"Term\\n\")\n",
    "# for i, item in enumerate(zip(tf_mat_unigram[idx_sample], idf_mat_unigram, tfidf_mat_unigram[idx_sample], terms_unigram)):\n",
    "#     if(item[2] != 0.0):\n",
    "#         print (\"array position \" + str(i) + \"\\t\", \n",
    "#                \"%.6f\" % item[0], \"\\t\", \n",
    "#                \"%.6f\" % item[1], \"\\t\", \n",
    "#                \"%.6f\" % item[2], \"\\t\", \n",
    "#                item[3])\n",
    "        \n",
    "def get_TF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [tf for tf in tf_mat_unigram[idx] if tf != 0.0]\n",
    "\n",
    "df[\"TF\"] = df.apply(get_TF_unigram, axis=1)\n",
    "\n",
    "def get_IDF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [item[1] for item in zip(tf_mat_unigram[idx], idf_mat_unigram) if item[0] != 0.0]\n",
    "\n",
    "df[\"IDF\"] = df.apply(get_IDF_unigram, axis=1)\n",
    "\n",
    "def get_TFIDF_unigram(row):\n",
    "    idx = row.name\n",
    "    return [tfidf for tfidf in tfidf_mat_unigram[idx] if tfidf != 0.0]\n",
    "\n",
    "df[\"TFIDF\"] = df.apply(get_TFIDF_unigram, axis=1)\n",
    "\n",
    "df[[\"final\", \"TF\", \"IDF\", \"TFIDF\"]]\n",
    "\n",
    "# save TFIDF Unigram to Excel\n",
    "\n",
    "# df[[\"final\", \"TF_UNIGRAM\", \"IDF_UNIGRAM\", \"TFIDF_UNIGRAM\"]].to_csv(\"TFIDF_Unigram.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1cf33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data\n",
    "def data_train():\n",
    "    data = pd.read_csv('label.csv')#ubah nama file sesai dengan nama file kalian\n",
    "    return data\n",
    "df_train = data_train()\n",
    "#definisi dataframe\n",
    "df_train = df_train.rename(columns={'final': 'caption'})\n",
    "df_train  = pd.DataFrame(df_train[['caption','sentiment']])\n",
    "df_train = df_train.dropna().reset_index(drop=True)#ubah dataframe sesuai column kalian\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(df_train[\"caption\"])\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(df_train[\"caption\"])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "df_train['encoding'] = enc.fit_transform(df_train['sentiment'])\n",
    "y_train = df_train['encoding']\n",
    "\n",
    "\n",
    "#Test Data\n",
    "# X_test_counts = count_vect.fit_transform(df[\"final\"])\n",
    "tfidf = TfidfVectorizer()\n",
    "X_test = tfidf.fit_transform(df[\"final\"])\n",
    "df['sentiment'] = ''\n",
    "y_test = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(tfs, target, test_size=0.2, stratify=target, random_state=85)\n",
    "print ((X_train.shape),(y_train.shape),(X_test.shape), (y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "#KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)  # n_neighbors means k\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "pred = enc.inverse_transform(Y_pred)\n",
    "print (pred)\n",
    "# predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# # evaluate predictions\n",
    "# accuracy = accuracy_score(Y_test, predictions)\n",
    "# accuracies['KNN'] = accuracy* 100.0\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# #try to find best k value\n",
    "# scoreList = []\n",
    "# for i in range(1,20):\n",
    "#     knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "#     knn2.fit(X_train, Y_train)\n",
    "#     scoreList.append(knn2.score(X_test, Y_test))\n",
    "    \n",
    "# plt.plot(range(1,20), scoreList)\n",
    "# plt.xticks(np.arange(1,20,1))\n",
    "# plt.xlabel(\"K value\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.show()\n",
    "\n",
    "# acc = max(scoreList)*100\n",
    "\n",
    "# print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "cluster = MongoClient(\"mongodb://db_hrcerdas:hrcerdas@hrcerdas.id:27017/?authMechanism=SCRAM-SHA-256\")\n",
    "db = cluster[\"testing\"]\n",
    "collection = db[\"testing\"]\n",
    "\n",
    "\n",
    "\n",
    "# results = collection.insert_many(df.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0rc2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0rc2"
  },
  "vscode": {
   "interpreter": {
    "hash": "37c0448c1c422973ba8fdc39db167944d77591df89dc93671090cd0e296c9cc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
